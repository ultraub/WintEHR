# Multi-LLM Provider Setup Guide

## Overview

The UI Composer now supports multiple LLM providers for experimental comparison of how different models interpret clinical requests and generate FHIR queries/UI components.

## Supported Providers

1. **Anthropic Claude** (Default)
   - Model: Claude 3.5 Sonnet
   - Uses existing integration

2. **OpenAI GPT-4**
   - Model: GPT-4 Turbo
   - Requires API key

3. **Azure OpenAI**
   - Model: GPT-4 (via Azure deployment)
   - Requires endpoint, key, and deployment name

4. **Google Gemini**
   - Model: Gemini 1.5 Pro
   - Requires API key

## Configuration

### Environment Variables

```bash
# Anthropic (already configured)
ANTHROPIC_API_KEY=your-anthropic-key

# OpenAI
OPENAI_API_KEY=your-openai-key

# Azure OpenAI
AZURE_OPENAI_API_KEY=your-azure-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT=your-deployment-name

# Google Gemini
GEMINI_API_KEY=your-gemini-key
```

### Installing Dependencies

```bash
pip install openai google-generativeai
```

## API Endpoints

### Check Provider Status
```
GET /api/ui-composer/providers/status
```

Returns availability of each provider:
```json
{
  "anthropic": {
    "available": true,
    "provider": "anthropic",
    "model": "claude-3-5-sonnet-20241022"
  },
  "openai": {
    "available": true,
    "provider": "openai",
    "model": "gpt-4-turbo-preview"
  },
  ...
}
```

### Compare Providers
```
POST /api/ui-composer/compare
```

Body:
```json
{
  "request": "Show patients with hypertension and their stroke risk",
  "context": {
    "user_role": "physician"
  },
  "providers": ["anthropic", "openai", "gemini"]
}
```

### Compare FHIR Query Generation
```
POST /api/ui-composer/compare/fhir-queries
```

Body:
```json
{
  "request": "Find all diabetic patients with recent HbA1c > 8",
  "available_resources": ["Patient", "Observation", "Condition"],
  "providers": ["anthropic", "openai"]
}
```

### Generate with Specific Provider
```
POST /api/ui-composer/generate-with-provider
```

Body:
```json
{
  "specification": {...},
  "fhir_data": {...},
  "provider": "gemini"
}
```

## Experimental Features

### 1. Provider Comparison Metrics
- Intent agreement across providers
- Data needs overlap percentage
- UI type consensus
- Response time comparison
- Query complexity analysis

### 2. FHIR Query Analysis
- Resource type agreement
- Query parameter differences
- Filtering strategy comparison
- Join/include pattern analysis

### 3. UI Generation Comparison
- Component structure differences
- Data binding approaches
- Error handling patterns
- Performance considerations

## Usage Examples

### Simple Comparison Test
```python
# Compare how different models interpret a clinical request
request = "Show patients at risk for sepsis based on recent lab values"

comparison = await unified_llm_service.analyze_request_comparison(
    request,
    {"user_role": "physician"},
    ["anthropic", "openai", "gemini"]
)

# Analyze agreement
print(f"Intent agreement: {comparison['comparison_metrics']['intent_agreement']}")
print(f"Data overlap: {comparison['comparison_metrics']['data_needs_overlap']}%")
```

### FHIR Query Generation Test
```python
# Compare FHIR queries generated by different models
clinical_request = "Patients with uncontrolled diabetes"

query_comparison = await unified_llm_service.generate_fhir_queries_comparison(
    clinical_request,
    ["Patient", "Condition", "Observation"],
    ["anthropic", "openai"]
)

# Compare query strategies
for provider, result in query_comparison['provider_results'].items():
    print(f"{provider}: {result['queries']}")
```

## Best Practices

1. **Start with 2-3 providers** for manageable comparisons
2. **Use consistent prompts** across providers for fair comparison
3. **Document differences** in interpretation and output
4. **Test with various clinical scenarios** to understand model strengths
5. **Monitor costs** as some providers charge per token

## Future Enhancements

1. **Automated testing suite** for clinical scenarios
2. **Performance benchmarks** across providers
3. **Cost optimization** based on query complexity
4. **Model-specific prompt engineering**
5. **Consensus mechanisms** for multi-model agreement